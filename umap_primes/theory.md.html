<meta charset="utf-8">
<!-- Markdeep: -->

.<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script>
window.markdeepOptions = {tocStyle:'none'};</script><script src="js/markdeep.min.js"></script>
<script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible");

</script>

# Theory and statistics

Can we analyse the layout further? In particular there are several questions that remain:

* Can we derive results about the layout for any N? For example, what distribution of clusters might we see.
* Can we identify which parts of this layout are specific to prime factorisation, and which are "common" properties?

One way to address these issues is to take a stochastic approach. This can either be approached analytically, looking at probabilistic number theory to determine factorisation relations among random integers and linking this to random graph theory and/or random matrix theory (e.g. what is the distribution of eigenvalues of the factorisation connectivity graph). 

There are also empirical approaches, where we can analyse the results obtained statistically as well as compare against surrogate datasets which have been simulated based on analytical or statistcal estimates (e.g. drawing a random sample from a random graph with the same edge weight distribution). 

UMAP is fundamentally a toplogical approach. Our argument might proceed:
* probabilistic number theory -> distributions over properties of random integers -> distributions over edge weights of a graph.
* random graph theory -> graph theoretic properties of this random graph -> properties of the simplicial complex UMAP identifies.
* [possibly] random matrix theory -> properties of the adjacency matrix of the graph

## Notation

* $i,j,n$ natural numbers, such that $i,j \leq n$
* $i^\prime$, the number $i$ with all repeated prime factors collapsed to an exponent of 1. For example if $i=12=2^2 \times 3$, then $i^\prime=2^1 \times 3^1=6$.
* $\omega(i)$ the count of unique prime factors of $i$
* $p(k)$ the [primorial](https://en.wikipedia.org/wiki/Primorial) function for index $k$
* $p_k$ the $k$th prime number.
* $\pi(n)$ the number of prime numbers $\leq n$.


## What is the transform doing?

We can see the transform as being analogous to a logarithmic transform, replacing a product of integers with a sum of vectors.

For a number $i$, we can see it as:

$$i = \prod_k=1^{\pi(i^\prime)} p_k^{a_k},$$ where $a_k$ are the corresponding exponents of each prime.

replacing with the collapsed version $i^\prime$:

$$i^\prime = \prod_k=1^{\pi(i^\prime)} b_k p_k,$$ where $b_k$ is a binary indicator variable (0 or 1).

$$\log(i^\prime) = \log(\sum_k=1^{\pi(i^\prime)} b_k p_k)$$

In the vector space representation, $i^\prime$ is represented as:
$$\mathbb{x_i} = \sum_k=1^{\pi(i^\prime) b_k x_k$$, where $x_k$ is the $k$th standard basis vector.



## Some statistics

We can put some simple bounds on the elements of vectors for a given limit $n$. 

* There will be $\pi(n)$ columns and $n$ rows.
* We can characterise the number of non-zero elements $r_i$ in each row $i$.
* Every row has at least one non-zero value (exactly one where prime) $r_i\geq 1$
* Any row can have at most as many non-zero values as the largest $k$ for which the [primorial](https://en.wikipedia.org/wiki/Primorial) $p(k)$ &lt; $i$

Therefore,
$$1\leq r_i \leq p(k)$$
The total number of non-zero vectors in the whole matrix is just $$\sum_{i=2}^n r_i.$$


$p(k)$ [is approximately](https://math.stackexchange.com/questions/106260/interpolating-the-primorial-p-n) 
$$ p(k) \approx e^{k\log(k)}$$

![Primorial function versus $e^{k \log k}$](imgs/primorial_approximation.svg)

#### Erdős–Kac
There is a very elegant result in probabilistic number theory, the [Erdős–Kac theorem](https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Kac_theorem). This, in summary, says that the number of unique prime factors of (large) $i$ is $$\omega(i) \sim \mathcal{N}(\log \log i, \sqrt{\log \log i}).$$ That is, the number of unique factors is a random variable normally distributed with mean $\log \log n$ and standard deviation $\sqrt{\log \log n}.$

![Erdos-Kac theorem. The empirical number of factors for a given number, averaged over a running window of 500 consecutive numbers, against $\log \log n$, with a constant of 0.31 added. There is a very close match.](imgs/erdos_kac.png)

It appears that the $\log log i$ result is close, but there is a constant offset of $\approx 0.31$.

### n=1000000
We can examine these properties for the 1M vector example.

* 100000 rows, 78498 columns, as $\pi(1000000)= 78498$.
* Min 1 non-zero element per row, max 7 elements per row.
* For $n=1000000$, the largest index is $k=7$ as $p_7=510510$ and $p_8=9699690$.
* Thus every row has between 1 and 7 non-zero elements.
* 0.003653% of all entries (2853710 of 78498000000) are non-zero
* Each row has mean 2.85 entries, std. dev. 0.99

By Erdős-Kac we would expect there to be  $$\frac{1}{n-2} \sum_{i=2}^{n} \mathbb{E}[\omega(i)] = \frac{1}{n-2} \sum_{i=2}^n \log \log i 
$$ entries. For 1M integers, this works out to an expected 2.54 entries per row or a matrix density of 0.002032%; a pretty good approximation!

#### Sparsity
It should be apparent that this space is extremely sparsely populated. There are $2^{78498}$ corners of the cube in which the data lies, of which only 2853710 (about $2^21$) are occupied. This is ignoring the interior volume of the cube, which is entirely empty.

### Distribution of distances

In the layout plotted originally, distances are computed with the cosine distance. One question that occurs is: what is the distribution of distances of pairs of these vectors under this metric? Since the layout generated by UMAP depends only on the inter-vector distances, this gives us some insight into the vectors we might expect. As discussed above, every vector corresponding to a prime is an orthogonal basis vector and has a cosine distance of 1.0 to every other prime (90 degrees separation). 

The distribution can be examined empirically. The plot below shows a histogram of the distribution of the cosine distances between pairs of vectors. It is based on a random sample of 20M pairs of (non-equal) rows from the 1M point dataset. There is a clear spike at 1.0, but an interesting distribution in between. 

![Distribution of cosine distances between vectors representing random integers from 2 to 1M, computed from 20M samples.](imgs/cosine_histogram.svg)


#### Distribution of GCD
Using the cosine distance, the vector norm will be: $$d(x_i, x_j) = 1 - \frac{x_i \cdot x_j}{\|x_i\|\|x_j\|}$$ Since the vectors are binary, this is:

$$d(x_i, x_j) = 1 - \frac{h(x_i, x_j)}{\sqrt{h(x_i)}\sqrt{h(x_j)}}$$
$$= 1 - \frac{h(x_i, x_j)}{\sqrt{\omega(i)}\sqrt{\omega(j)}}.$$

where $h(x_i, x_j)$ is the Hamming distance of $x_i$ and $x_j$ and $h(x_i)$ is the shorthand for $h(x_i,0)$. $h(x_i)$ is equal to the number of distinct prime factors of $i$, written $\omega(i)$. The Hamming distance is just the count of the difference in factors. This is equal to the number of unique factors in the greatest common divisor of $i$ and $j$, $\omega(\gcd(i,j))$.

Therefore a the distance $d(i,j)$ (as described above) for two numbers $i, j$ is: 
$$d(i,j)= 1-\frac{\omega(\gcd(i, j))}{\sqrt{\omega(i)}\sqrt{\omega(j)}}.$$  

Correspondingly, if we view the problem as a graph, then the edge between vertex $i$ and $j$ is:
$$\w_{ij}= \frac{\omega(\gcd(i, j))}{\sqrt{\omega(i)}\sqrt{\omega(j)}},$$  changing the distance to a similarity.


The distribution of  $\gcd(i,j)$ for random integers $i, j$ is known [Fernandez 2013](https://arxiv.org/pdf/1305.0536.pdf) and [Diaconis and Erdos 2004](https://projecteuclid.org/download/pdf_1/euclid.lnms/1196285379). It has PMF:

$$ 
P(\gcd(i,j)=k) = \frac{1}{\zeta(2)k^2} + O\left(\frac{1+\log(n+k)}{nk}\right), \quad 1 \leq i,j,k \leq n
$$

and expectation:

$$ \mathbb{E}(\gcd(i,j)) = \frac{1}{\zeta(2)} \log(n) + C + O\left(\frac{1+\log(n/k)}{\sqrt{n}}\right), \quad 1 \leq i,j \leq n$$
$$  = \frac{6}{\pi^2} \log(n) + C + O\left(\frac{1+\log(n/k)}{\sqrt{n}}\right),$$
$$ \approx \frac{6}{\pi^2} \log(n), $$

![Predicted GCD of random integers $i,j \sim U(2,n)$ and sample mean of the GCD of 2000 random integer pairs $\leq n$ for various $n$. The approximation is a reasonable fit.](imgs/predicted_gcd.svg)

Assuming that $i,j \sim U(2,n)$, and with $\mathbb{E}[\omega(i)] = \log \log(i)$  we can 
therefore derive the expected value of the distances to be:

$$\mathbb{E}(d(i,j)) \approx 1-\frac{\log\log \left( \frac{6\log(n)}{\pi^2} \right)}{\sqrt{\log \log i}\sqrt{\log \log j}}, \quad 1 \leq i,j \leq n$$
$$\mathbb{E}(d(n)) \approx 1-\frac{\log\log \left( \frac{6\log(n)}{\pi^2} \right)} {\sqrt{\log \log (n/2)}\sqrt{\log \log (n/2)}}$$
$$ \approx 1-\frac{\log\log \left( \frac{6\log(n)}{\pi^2} \right)} {\log \log (n/2)}$$

where $\mathbb{E[d(n)]}$ means the expected cosine distance of two numbers $i,j \sim U(2,n)$.
